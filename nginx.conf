worker_processes 16;
pid /var/run/nginx.pid;

events {
    worker_connections 5000;
    # multi_accept on;
}


http {

    ##
    # Basic Settings
    ##

    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    # server_tokens off;

    server_names_hash_bucket_size 128;
    # server_name_in_redirect off;


    ##
    # Logging Settings
    ##

    log_format  main  '$remote_addr $host $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" $upstream_cache_status' ;
    access_log /var/log/static/access.log main;
    error_log /var/log/static/error.log;

    ##
    # Gzip Settings
    ##

    gzip on;
    gzip_disable "msie6";

    # gzip_vary on;
    # gzip_proxied any;
    # gzip_comp_level 6;
    # gzip_buffers 16 8k;
    # gzip_http_version 1.1;
    # gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;



    ##
    # Virtual Host Configs
    ##

    real_ip_header X-Forwarded-For;
    set_real_ip_from 10.0.0.0/8;

    proxy_cache_path  /nginx-cache/  levels=1:2 keys_zone=STATIC:10m inactive=24h  max_size=1g;
    server {
        listen 80;
        include /etc/nginx/proxy.conf;
    }

    # Projects where most of the subject URLs are hard-coded to S3.
    # We need to redirect the few that use the project domain instead.
    server {
        server_name www.galaxyzoo.org www.seafloorexplorer.org;

        location /subjects/ {
            return 301 http://$host.s3.amazonaws.com$request_uri;
        }

        include /etc/nginx/proxy.conf;
    }

    # Elasticsearch proxy for log viewer
    server {
        server_name logs.zooniverse.org;

        location /es/ {
            proxy_pass http://es.zooniverse.org/;
        }

        include /etc/nginx/proxy.conf;
    }

    include /etc/nginx/redirects.conf;
}
